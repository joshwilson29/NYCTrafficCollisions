{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import kc_functions as kc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('data/2014/accident.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getDistFromRow(dftmp,loc):\n",
    "#     location=['LONGITUD','LATITUDE']\n",
    "#     return np.linalg.norm(dftmp.loc[:,location]-loc,axis=1)\n",
    "# \n",
    "# def LookupLocations(state,county,city,dflookup):\n",
    "#     if dflookup is None:\n",
    "#         dflookup=pd.read_csv(basefile,sep='|')\n",
    "#     return dflookup.loc[(dflookup.STATE_NUMERIC==state) & (dflookup.COUNTY_NUMERIC==county),['PRIM_LONG_DEC','PRIM_LAT_DEC']].median()\n",
    "# def fillLocations(dftmp,basefile='data/NationalFile_20190501.txt'):\n",
    "#     dflup=pd.read_csv(basefile,sep='|')\n",
    "#     state=dftmp.columns.tolist('STATE')\n",
    "#     county=dftmp.columns.tolist('COUNTY')\n",
    "#     city=dftmp.columns.tolist('CITY')\n",
    "#     for i,v in dftmp.loc[(dftmp.LATITUDE>=99) & (dftmp.LONGITUD>=999),:].iterrows():\n",
    "#         loc=LookupLocations(state,county,city,dflup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize, cluster, and test for significance example - workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(kc)\n",
    "\n",
    "df=kc.getalldata()\n",
    "# need to add a merge to figure out how to merge sub-tables together into 1 row\n",
    "df=kc.removeNoHourAndMinutes(df)\n",
    "df=kc.createTimestamp(df)\n",
    "df=kc.removeUnneededColumns(df)\n",
    "df=kc.cleanLocs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>state</td>\n",
       "      <td>0.394148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>county</td>\n",
       "      <td>0.141392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>city</td>\n",
       "      <td>0.539595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peds</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>route</td>\n",
       "      <td>0.950530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>harm_ev</td>\n",
       "      <td>0.760884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>man_coll</td>\n",
       "      <td>0.994230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reljct2</td>\n",
       "      <td>0.977835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>typ_int</td>\n",
       "      <td>0.998123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lgt_cond</td>\n",
       "      <td>0.996607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>weather</td>\n",
       "      <td>0.989498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cf1</td>\n",
       "      <td>0.982028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>drunk_dr</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    variable   percent\n",
       "0      state  0.394148\n",
       "1     county  0.141392\n",
       "2       city  0.539595\n",
       "3       peds  0.999700\n",
       "4      route  0.950530\n",
       "5    harm_ev  0.760884\n",
       "6   man_coll  0.994230\n",
       "7    reljct2  0.977835\n",
       "8    typ_int  0.998123\n",
       "9   lgt_cond  0.996607\n",
       "10   weather  0.989498\n",
       "11       cf1  0.982028\n",
       "12  drunk_dr  1.000000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kc.calculateTopNCatPct(df)\n",
    "reload(kc)\n",
    "kc.calculateTopNCatPct(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>34.6237</td>\n",
       "      <td>34.3974</td>\n",
       "      <td>33.1972</td>\n",
       "      <td>33.1964</td>\n",
       "      <td>34.1802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitud</th>\n",
       "      <td>-85.9814</td>\n",
       "      <td>-87.7721</td>\n",
       "      <td>-87.5259</td>\n",
       "      <td>-86.3078</td>\n",
       "      <td>-86.7846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>county</th>\n",
       "      <td>71</td>\n",
       "      <td>59</td>\n",
       "      <td>125</td>\n",
       "      <td>121</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3050</td>\n",
       "      <td>2275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peds</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harm_ev</th>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man_coll</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reljct2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typ_int</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgt_cond</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cf1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drunk_dr</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>id_2014.10001</td>\n",
       "      <td>id_2014.10002</td>\n",
       "      <td>id_2014.10003</td>\n",
       "      <td>id_2014.10004</td>\n",
       "      <td>id_2014.10005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0              1              2              3  \\\n",
       "latitude        34.6237        34.3974        33.1972        33.1964   \n",
       "longitud       -85.9814       -87.7721       -87.5259       -86.3078   \n",
       "state                 1              1              1              1   \n",
       "county               71             59            125            121   \n",
       "city                  0              0           3050           2275   \n",
       "day                   1              1              1              2   \n",
       "month                 1              1              1              1   \n",
       "hour                  1             13              3              9   \n",
       "peds                  0              0              0              0   \n",
       "route                 4              4              2              2   \n",
       "harm_ev              17             42             12             12   \n",
       "man_coll              0              0              6              2   \n",
       "reljct2               1              1              2              1   \n",
       "typ_int               1              1              2              1   \n",
       "lgt_cond              2              1              3              1   \n",
       "weather               1              1              1              2   \n",
       "cf1                   0              0              0              0   \n",
       "drunk_dr              1              1              0              0   \n",
       "id        id_2014.10001  id_2014.10002  id_2014.10003  id_2014.10004   \n",
       "\n",
       "                      4  \n",
       "latitude        34.1802  \n",
       "longitud       -86.7846  \n",
       "state                 1  \n",
       "county               43  \n",
       "city                  0  \n",
       "day                   2  \n",
       "month                 1  \n",
       "hour                 16  \n",
       "peds                  0  \n",
       "route                 4  \n",
       "harm_ev              35  \n",
       "man_coll              0  \n",
       "reljct2               1  \n",
       "typ_int               1  \n",
       "lgt_cond              1  \n",
       "weather               1  \n",
       "cf1                   0  \n",
       "drunk_dr              0  \n",
       "id        id_2014.10005  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=kc_functions.binarizeVariables(df)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28961038\n"
     ]
    }
   ],
   "source": [
    "print(df.memory_usage(deep=True).sum())\n",
    "#df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(kc)\n",
    "hdbscan_params={'min_cluster_size':100,'gen_min_span_tree':False, 'metric':'manhattan'}\n",
    "dfwk=kc.cluster_all_points(df,df.harm_ev==1,hdbscan_params=hdbscan_params)\n",
    "dfwk=kc.ttest(dfwk,dfwk.harm_ev==1)\n",
    "dfwk=kc.signficant_clusters(dfwk,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(kc_functions)\n",
    "# kc_functions.output_notebook()\n",
    "#kc_functions.plot_map(dfwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.plot_map(dfwk,dfwk.cluster,dfwk.harm_ev==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwk=dfwk.loc[:,['id','cluster','label','tstat','pvalue','sig']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_cluster_size': 100, 'metric': 'manhattan', 'min_samples': 100}\n"
     ]
    }
   ],
   "source": [
    "# import tabpy as tabpy\n",
    "reload(tabpy)\n",
    "\n",
    "sigcluster,cluster,pvalue=tabpy.get_sig_clusters(df.longitud.values,df.latitude.values,(df.harm_ev==1).values,min_cluster_size=100,test_type='ttest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(dfwk['cluster']==sigcluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-271-33506fa9ce1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpvalue\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'float'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HDBSCAN(algorithm='best', allow_single_cluster=False, alpha=1.0,\n",
       "    approx_min_span_tree=True, cluster_selection_method='eom',\n",
       "    core_dist_n_jobs=4, gen_min_span_tree=True, leaf_size=40,\n",
       "    match_reference_implementation=False, memory=Memory(location=None),\n",
       "    metric='manhattan', min_cluster_size=300, min_samples=None, p=None,\n",
       "    prediction_data=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdbscan\n",
    "\n",
    "dfwk=df.loc[df.WRK_ZONE==1,LOCS]\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=300, gen_min_span_tree=True,metric='manhattan')\n",
    "clusterer.fit(dfwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_map(dfwk,clusterer.labels_,title='Fatal Accident Road condition Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow example\n",
    "# z=cluster_all_points(df,df.WRK_ZONE==1)\n",
    "# z=ttest(z,z.WRK_ZONE==1)\n",
    "# z=signficant_clusters(z,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=getalldata()\n",
    "#df=mergedata(df)\n",
    "#df=cleanDFLocs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwk.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwk=create_plot_labels(dfwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(dfwk,dfwk.plot_labels,title='Fatal Accident Road condition Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_map(dftmp,dftmp.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=dfwk.loc[dfwk.cluster==3,LOCS]\n",
    "hull=convexhull(z)\n",
    "#hull=spatial.ConvexHull(z)\n",
    "hull.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "points=z.loc[:,LOCS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points[hull.vertices,0], points[hull.vertices,1], 'r--', lw=2)\n",
    "plt.plot(points[hull.vertices[0],0], points[hull.vertices[0],1], 'ro')\n",
    "plt.plot(points[:,0],points[:,1],'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=df.loc[hull.in_hull(df.loc[:,LOCS].values),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ttest_ind(z.accident_WRK_ZONE,dfwk.loc[dfwk.cluster==-1,'accident_WRK_ZONE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCS=['accident_LONGITUD', 'accident_LATITUDE','cluster']\n",
    "# z.loc[:,LOCS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accident_LONGITUD</th>\n",
       "      <th>accident_LATITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-85.981408</td>\n",
       "      <td>34.623722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-87.772117</td>\n",
       "      <td>34.397428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-87.525911</td>\n",
       "      <td>33.197172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-86.307831</td>\n",
       "      <td>33.196383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-86.784592</td>\n",
       "      <td>34.180189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accident_LONGITUD  accident_LATITUDE\n",
       "0         -85.981408          34.623722\n",
       "1         -87.772117          34.397428\n",
       "2         -87.525911          33.197172\n",
       "3         -86.307831          33.196383\n",
       "4         -86.784592          34.180189"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,LOCS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df.loc[~hull.in_hull(df.loc[:,LOCS].values),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[~hull.in_hull(df.loc[:,LOCS].values),'accident_WRK_ZONE'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=getalldata()\n",
    "# need to add a merge to figure out how to merge sub-tables together into 1 row\n",
    "df=removeNoHourAndMinutes(df)\n",
    "df=removeUneededAccidentColumns(df)\n",
    "df=cleanLocs(df)\n",
    "df=createTimestamp(df)\n",
    "\n",
    "df2=getalldata()\n",
    "# need to add a merge to figure out how to merge sub-tables together into 1 row\n",
    "df2=removeNoHourAndMinutes(df2)\n",
    "df2=removeUneededAccidentColumns(df2)\n",
    "df2=cleanLocs(df2)\n",
    "df2=createTimestamp(df2)\n",
    "df=df.append(df2)\n",
    "df=df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.shape)\n",
    "# print(len(df.reset_index()['index'].unique()))\n",
    "# print(len(df.reset_index().drop('index',axis=1).reset_index()['index'].unique()))\n",
    "# df.sort_values('id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(kc_functions)\n",
    "df=kc_functions.getalldata()\n",
    "df=kc_functions.removeNoHourAndMinutes(df)\n",
    "df=kc_functions.createTimestamp(df)\n",
    "df=kc_functions.removeUneededColumns(df)\n",
    "df['target']=df['drunk_dr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 9, 3, 5, 1, 8, 2, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import kc_functions\n",
    "df=pd.read_csv('comboacc.csv')\n",
    "df.columns=[ i.lower() for i in df.columns.tolist()]\n",
    "#df=kc_functions.removeNoHourAndMinutes(df)\n",
    "#df=kc_functions.cleanLocs(df)\n",
    "df['target']=df['sp_jur']\n",
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "QhullError",
     "evalue": "QH6013 qhull input error: input is less than 2-dimensional since it has the same x coordinate\n\nWhile executing:  | qhull i Qt\nOptions selected for Qhull 2015.2.r 2016/01/18:\n  run-id 1991766488  incidence  Qtriangulate  _pre-merge  _zero-centrum\n  _max-width  0  Error-roundoff 7.2e-13  _one-merge 3.6e-12\n  _near-inside 1.8e-11  Visible-distance 1.4e-12  U-coplanar-distance 1.4e-12\n  Width-outside 2.9e-12  _wide-facet 8.6e-12\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQhullError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-92d8d3064184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mdftgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniquedf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muniquedf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"longitud\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"latitude\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mhull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConvexHull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdftgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mhull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0muniquedf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_hull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhull\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muniquedf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"longitud\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"latitude\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mqhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull.ConvexHull.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mqhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull._Qhull.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mQhullError\u001b[0m: QH6013 qhull input error: input is less than 2-dimensional since it has the same x coordinate\n\nWhile executing:  | qhull i Qt\nOptions selected for Qhull 2015.2.r 2016/01/18:\n  run-id 1991766488  incidence  Qtriangulate  _pre-merge  _zero-centrum\n  _max-width  0  Error-roundoff 7.2e-13  _one-merge 3.6e-12\n  _near-inside 1.8e-11  Visible-distance 1.4e-12  U-coplanar-distance 1.4e-12\n  Width-outside 2.9e-12  _wide-facet 8.6e-12\n"
     ]
    }
   ],
   "source": [
    "# ```SCRIPT_INT('from scipy.spatial import Delaunay, ConvexHull\n",
    "from scipy.spatial import Delaunay,ConvexHull\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.spatial.qhull import QhullError\n",
    "import pandas as pd\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "\n",
    "def in_hull(h,p):\n",
    "    hull_in=Delaunay(h.points)\n",
    "    return hull_in.find_simplex(p)>=0\n",
    "\n",
    "LOCS=df.loc[:,[\"longitud\",\"latitude\",\"target\"]]\n",
    "# Assuming each row is unique\n",
    "#LOCS = np.column_stack([np.radians(_arg1),np.radians(_arg2), _arg3]) \n",
    "uniquedf=pd.DataFrame(LOCS,columns=[\"longitud\",\"latitude\",\"target\"])\n",
    "# uniquedf is the uniques\n",
    "targetdf = uniquedf.loc[uniquedf[\"target\"]==1,:]\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=500,gen_min_span_tree=True,metric=\"manhattan\",min_samples=500)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5,gen_min_span_tree=True,metric=\"manhattan\",min_samples=5)\n",
    "clustered = clusterer.fit(targetdf.loc[:,[\"longitud\",\"latitude\"]])\n",
    "targetdf[\"cluster\"]=clustered.labels_\n",
    "uniquedf[\"cluster\"]=0\n",
    "# relabel cluster 0 so when we multiply it out to indicate significant clusters, it works - adding 1 to everything\n",
    "uniquedf.loc[list(targetdf.index),\"cluster\"]=clustered.labels_ + 1\n",
    "for i in uniquedf[\"cluster\"].unique():\n",
    "    if i!=0:\n",
    "        dftgt=uniquedf.loc[uniquedf[\"cluster\"]==i,[\"longitud\",\"latitude\"]]\n",
    "        try:\n",
    "            hull=ConvexHull(dftgt)\n",
    "        except Exception as Err:\n",
    "            hull=None\n",
    "            if type(Err) == QhullError:\n",
    "                if len(dftgt.longitud.unique())<3 and len(dftgt.latitude.unique())<3:\n",
    "                    for lon,lat in zip(dftgt.longitud.unique(),dftgt.latitude.unique()):\n",
    "                        uniquedf.loc[(uniquedf.longitud==lon) & (uniquedf.latitude==lat),\"cluster\"]==i\n",
    "                else:\n",
    "                    print(Err.args)   # True error here - we\"ll have ot know about about it - not sure how you display it in tableau\n",
    "        if hull is not None:\n",
    "            hull.close()\n",
    "            uniquedf.loc[in_hull(hull,uniquedf.loc[:,[\"longitud\",\"latitude\"]].values),\"cluster\"]=i\n",
    "# ttest - no need for any correction because it\"s different data sets\n",
    "dfnotincluster=uniquedf.loc[uniquedf[\"cluster\"]==0,\"target\"]\n",
    "uniquedf[\"tstat\"]=-1\n",
    "uniquedf[\"pvalue\"]=-1\n",
    "for i in uniquedf[\"cluster\"].unique():\n",
    "    if i!=0:\n",
    "        dfcluster=uniquedf.loc[uniquedf[\"cluster\"]==i,\"target\"]\n",
    "        tstat,pvalue=ttest_ind(dfcluster,dfnotincluster)\n",
    "        uniquedf.loc[uniquedf[\"cluster\"]==i,[\"tstat\"]]=tstat\n",
    "        uniquedf.loc[uniquedf[\"cluster\"]==i,[\"pvalue\"]]=pvalue\n",
    "# now, check for significance\n",
    "# uniquedf[\"sig\"]=uniquedf.pvalue<= 0.05 \n",
    "uniquedf[\"sig\"]=uniquedf.pvalue<=.05\n",
    "uniquedf[\"sig_cluster\"]=uniquedf.cluster*uniquedf.sig\n",
    "\n",
    "\n",
    "# Showing certain clusters show up as significant that have no difference (likely due to chance). \n",
    "# ',\n",
    "# avg([Longitud]), \n",
    "# avg([Latitude]),\n",
    "# avg([Measure Chooser]),\n",
    "# min([# of clusters]))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Final for Tabpy\n",
    "\n",
    "from scipy.spatial import Delaunay, ConvexHull\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.spatial.qhull import QhullError\n",
    "import pandas as pd\n",
    "import hdbscan\n",
    "\n",
    "def in_hull(h,p):\n",
    "    \"\"\"\n",
    "    Test if points in `p` are in `hull`\n",
    "\n",
    "    `p` should be a `NxK` coordinates of `N` points in `K` dimensions\n",
    "    `hull` is either a scipy.spatial.Delaunay object or the `MxK` array of the \n",
    "    coordinates of `M` points in `K`dimensions for which Delaunay triangulation\n",
    "    will be computed\n",
    "    \"\"\"\n",
    "    hull_in=Delaunay(h.points)\n",
    "    return hull_in.find_simplex(p)>=0\n",
    "\n",
    "LOCS=df.loc[:,['longitud','latitude','target']]\n",
    "# Assuming each row is unique\n",
    "# TABPY LOCS = np.column_stack([np.radians(_arg1),np.radians(_arg2), _arg3]) \n",
    "uniquedf=pd.DataFrame(LOCS,columns=['longitud','latitude','target'])\n",
    "# uniquedf is the uniques\n",
    "targetdf = uniquedf.loc[uniquedf['target']==1,:]\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5,gen_min_span_tree=True,metric=\"manhattan\",min_samples=5)\n",
    "#clusterer = hdbscan.HDBSCAN(min_cluster_size=_arg4[1],gen_min_span_tree=True,metric=\"manhattan\",min_samples=_arg5[1])\n",
    "clustered = clusterer.fit(targetdf.loc[:,['longitud','latitude']])\n",
    "targetdf['cluster']=clustered.labels_\n",
    "uniquedf['cluster']=0\n",
    "# relabel cluster 0 so when we multiply it out to indicate significant clusters, it works - adding 1 to everything\n",
    "uniquedf.loc[list(targetdf.index),'cluster']=clustered.labels_ + 1\n",
    "for i in uniquedf['cluster'].unique():\n",
    "    if i!=0:\n",
    "        dftgt=uniquedf.loc[uniquedf['cluster']==i,['longitud','latitude']]\n",
    "        try:\n",
    "            hull=ConvexHull(dftgt)\n",
    "        except Exception as Err:\n",
    "            hull=None\n",
    "            if type(Err) == QhullError:\n",
    "                if len(dftgt.longitud.unique())<3 and len(dftgt.latitude.unique())<3:\n",
    "                    for lon,lat in zip(dftgt.longitud.unique(),dftgt.latitude.unique()):\n",
    "                        uniquedf.loc[(uniquedf.longitud==lon) & (uniquedf.latitude==lat),'cluster']==i\n",
    "                else:\n",
    "                    print(Err.args)   # True error here - we'll have ot know about about it - not sure how you display it in tableau\n",
    "        if hull is not None:\n",
    "            hull.close()\n",
    "            uniquedf.loc[in_hull(hull,uniquedf.loc[:,['longitud','latitude']].values),'cluster']=i\n",
    "# ttest - no need for any correction because it's different data sets\n",
    "dfnotincluster=uniquedf.loc[uniquedf['cluster']==0,'target']\n",
    "uniquedf['tstat']=-1\n",
    "uniquedf['pvalue']=-1\n",
    "for i in uniquedf['cluster'].unique():\n",
    "    if i!=0:\n",
    "        dfcluster=uniquedf.loc[uniquedf['cluster']==i,'target']\n",
    "        tstat,pvalue=ttest_ind(dfcluster,dfnotincluster)\n",
    "        uniquedf.loc[uniquedf['cluster']==i,['tstat']]=tstat\n",
    "        uniquedf.loc[uniquedf['cluster']==i,['pvalue']]=pvalue\n",
    "# now, check for significance\n",
    "uniquedf['sig']=uniquedf.pvalue<= 0.03 \n",
    "# TABPY uniquedf['sig']=uniquedf.pvalue<=_arg6[1]\n",
    "uniquedf['sig_cluster']=uniquedf.cluster*uniquedf.sig\n",
    "\n",
    "# PLEASE READ: basically cluster 0 means it's not a cluster - everythign else, it's in a cluster\n",
    "# fulldf has every single point (repeated) - cluster is the cluster it is in 0 means not in a cluster or significant cluster, -2 means repeated\n",
    "# uniquedf has only the unique points - sig_cluster is the significant cluster 0 means it's not in a cluster, cluster = cluster from hdbscan (no significance)\n",
    "# return uniquedf['sig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.output_notebook()\n",
    "# kc_functions.plot_map(uniquedf,uniquedf.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.plot_map(uniquedf,uniquedf.sig_cluster) #,uniquedf.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist=getVarlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(kc_functions)\n",
    "# a=kc_functions.createHistograms('accident.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should probably just bin the Vehicle Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=getalldata('person.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38010776, 0.65283572, 0.82420263, 0.90564561, 0.94857473,\n",
       "       0.97152498, 0.98336487, 0.99001436, 0.9934417 , 0.99558474,\n",
       "       0.99693743, 0.99779617, 0.99825973, 0.99858651, 0.99881449,\n",
       "       0.99901208, 0.99920966, 0.99932365, 0.99939965, 0.99946804,\n",
       "       0.99952124, 0.99957443, 0.99960483, 0.99963523, 0.99966563,\n",
       "       0.99969602, 0.99971882, 0.99974162, 0.99976442, 0.99977962,\n",
       "       0.99979482, 0.99981001, 0.99982521, 0.99984041, 0.99985561,\n",
       "       0.99987081, 0.99988601, 0.99990121, 0.99991641, 0.99992401,\n",
       "       0.99993161, 0.9999392 , 0.9999468 , 0.9999544 , 0.999962  ,\n",
       "       0.9999696 , 0.9999772 , 0.9999848 , 0.9999924 , 1.        ])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=df.loc[:,varlist['person']].groupby('id')['per_no'].count()\n",
    "(z.value_counts()/z.shape[0]).values.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.output_notebook()\n",
    "# kc_functions.createHistograms('accident.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.createHistograms('damage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.createHistograms('vision.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.createHistograms('drimpair.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.createHistograms('factor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.createHistograms('person.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_functions.createHistograms('acc_aux.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bokeh.models.annotations import Span\n",
    "# \n",
    "# k=[i/100 for i in range(0,int(2*np.pi*100),10)]\n",
    "# z=np.sin(k)\n",
    "# \n",
    "# p=kc_functions.figure(tooltips=[('x','$x'),('y','$y')])\n",
    "# p.line(x=k,y=z,legend='sin')\n",
    "# p.circle(x=k,y=z,legend='sin',radius=0.10,fill_color=None)\n",
    "# s=Span(location=0,dimension='width',line_width=1,line_color='darkred',line_dash='dotted')\n",
    "# p.add_layout(s)\n",
    "# s=Span(location=3,dimension='height',line_width=1,line_color='navy',line_dash='dashed')\n",
    "# p.add_layout(s)\n",
    "# kc_functions.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Functions to merge data for EDA in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3209: DtypeWarning: Columns (36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n",
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3209: DtypeWarning: Columns (36,37,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n",
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3209: DtypeWarning: Columns (101,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "# load values for R\n",
    "import kc_functions\n",
    "from importlib import reload\n",
    "reload(kc_functions)\n",
    "accident=kc_functions.getalldata('accident.csv')\n",
    "accident=kc_functions.removeNoHourAndMinutes(accident)\n",
    "accident=kc_functions.cleanLocs(accident)\n",
    "accident=kc_functions.createTimestamp(accident)\n",
    "accident=kc_functions.removeUnneededColumns(accident,'accident.csv',['tstamp'])\n",
    "vehicle=kc_functions.getalldata('vehicle.csv')\n",
    "vehicle=kc_functions.removeUnneededColumns(vehicle,'vehicle.csv')\n",
    "veh_aux=kc_functions.getalldata('veh_aux.csv')\n",
    "veh_aux=kc_functions.removeUnneededColumns(veh_aux,'veh_aux.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/pandas/core/indexing.py:1017: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return getattr(section, self.name)[new_key]\n"
     ]
    }
   ],
   "source": [
    "accident=accident.set_index('id')\n",
    "vehicle.loc[:,'tstamp']=accident.loc[list(vehicle.id),'tstamp'].values\n",
    "vehicle.loc[:,'state']=accident.loc[list(vehicle.id),'state'].values\n",
    "vehicle.loc[:,'longitud']=accident.loc[list(vehicle.id),'longitud'].values\n",
    "vehicle.loc[:,'latitude']=accident.loc[list(vehicle.id),'latitude'].values\n",
    "veh_aux.loc[:,'state']=accident.loc[list(veh_aux.id),'state'].values\n",
    "veh_aux.loc[:,'tstamp']=accident.loc[list(veh_aux.id),'tstamp'].values\n",
    "veh_aux.loc[:,'longitud']=accident.loc[list(veh_aux.id),'longitud'].values\n",
    "veh_aux.loc[:,'latitude']=accident.loc[list(veh_aux.id),'latitude'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle=vehicle.loc[(~vehicle.tstamp.isnull()) | (~vehicle.longitud.isnull()) | (~vehicle.latitude.isnull()),:]\n",
    "veh_aux=veh_aux.loc[(~veh_aux.tstamp.isnull()) | (~veh_aux.longitud.isnull()) | (~veh_aux.latitude.isnull()),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.to_csv('kc_eda_vehicle.csv')\n",
    "veh_aux.to_csv('kc_eda_veh_aux.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the EDA is done in R\n",
    "\n",
    "### EDA Tranformations below\n",
    "\n",
    "We'll code only examining vehicles 1-4 (~94% of all accidents) and vehicles 1-4 covers 98-99% of variables\n",
    "\n",
    "We'll code each categorical variable as veh_no_(1-4)_categorical_var_1,veh_no_(1-4)_categorical_var_2, and so on and numerical as veh_no_(1-4)_numericVar.\n",
    "\n",
    "Note veh_aux doesn't have vehicle number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import kc_functions as kc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dforig=pd.read_csv('kc_eda_vehicle.csv')\n",
    "# df=dforig.copy()\n",
    "# cols=[i for i in df.columns.tolist() if i not in ['tstamp','longitud','latitude','Unnamed: 0']]\n",
    "# df=df.loc[:,cols]\n",
    "# df=df.loc[(df.veh_no<=4),:]\n",
    "# df['veh_no']=['veh'+str(i) for i in df.veh_no]\n",
    "# df=df.drop('state',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'kc_functions' from '/home/kchuangk/Documents/Classes/W210 - Capstone/NYCTrafficCollisions/kc/kc_functions.py'>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import numpy as np\n",
    "import kc_functions as kc\n",
    "reload(kc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3248: DtypeWarning: Columns (36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3248: DtypeWarning: Columns (36,37,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3248: DtypeWarning: Columns (101,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['underride' 'vpavetype'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-45e036175a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvehicle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetalldata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vehicle.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvehicle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveUnneededColumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'vehicle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvehicle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unittype'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'spec_use'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'underride'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vpavetype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# columns dropped from lack of variance in additional EDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# map the variables into bins/translations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvehicle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mveh_no\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/.pyenv/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['underride' 'vpavetype'] not found in axis\""
     ]
    }
   ],
   "source": [
    "reload(kc)\n",
    "#kc.createDataDictForTranslation()\n",
    "vehicle=kc.getalldata('vehicle.csv')\n",
    "vehicle=kc.removeUnneededColumns(vehicle,'vehicle')\n",
    "vehicle=vehicle.drop(['unittype','spec_use','underride', 'vpavetype'],axis=1)   # columns dropped from lack of variance in additional EDA\n",
    "# map the variables into bins/translations\n",
    "vehicle=vehicle.loc[(vehicle.veh_no<=4),:]\n",
    "vehicle=kc.translateVarsFromDict(vehicle,kc.createDataDictForTranslation('vehicle'))\n",
    "vehicle['veh_no']=['veh'+str(int(i)) for i in vehicle.veh_no]\n",
    "vehicle=kc.pivot_and_chunk(vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove variables not to expand\n",
    "cols=vehicle.columns.tolist()\n",
    "for k,v in kc.createDataDictForTranslation().items():   # filtering out only columns to be binarized\n",
    "    if k in kc.checkDataDictForType(SourceFile='vehicle',return_type='numeric'):\n",
    "        for i in range(1,5):\n",
    "            cols.remove(k+'$veh'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the variables\n",
    "vehicle=kc.binarizeVariables(vehicle,cols,500)\n",
    "# vehicles is ready for merging\n",
    "vehicle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PLEASE LOOK - Use the workflow below to process to file as a template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'kc_functions' from '/home/kchuangk/Documents/Classes/W210 - Capstone/NYCTrafficCollisions/kc/kc_functions.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import kc_functions as kc\n",
    "reload(kc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3248: DtypeWarning: Columns (36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3248: DtypeWarning: Columns (36,37,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3248: DtypeWarning: Columns (101,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kchuangk/Documents/Classes/W210 - Capstone/NYCTrafficCollisions/kc/kc_functions.py:490: RuntimeWarning: Mean of empty slice\n",
      "  dftmp[z.group(1)+'$'+z.group(2) + '$' + c]=f(r,axis=1)\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import kc_functions as kc\n",
    "\n",
    "reload(kc)\n",
    "#kc.createDataDictForTranslation()\n",
    "vehicle=kc.getalldata('vehicle.csv')\n",
    "vehicle=kc.removeUnneededColumns(vehicle,'vehicle')\n",
    "vehicle=vehicle.drop(['unittype','spec_use','underide','vpavetyp'],axis=1)   # columns dropped from lack of variance in additional EDA\n",
    "# map the variables into bins/translations\n",
    "vehicle=vehicle.loc[(vehicle.veh_no<=4),:]\n",
    "vehicle=kc.translateVarsFromDict(vehicle,kc.createDataDictForTranslation('vehicle'))\n",
    "vehicle['veh_no']=['veh'+str(int(i)) for i in vehicle.veh_no]\n",
    "vehicle=kc.pivot_and_chunk(vehicle)\n",
    "# remove variables not to expand\n",
    "cols=vehicle.columns.tolist()\n",
    "for k,v in kc.createDataDictForTranslation().items():   # filtering out only columns to be binarized\n",
    "    if k in kc.checkDataDictForType(SourceFile='vehicle',return_type='numeric'):\n",
    "        for i in range(1,5):\n",
    "            cols.remove(k+'$veh'+str(i))\n",
    "vehicle=kc.binarizeVariables(vehicle,cols,500)    # binarize the columns\n",
    "vehicle=kc.addSummaryStats(vehicle,functions=[np.nanmean,np.nansum,np.any],postfixes=['mean','sum','any']).replace({True:1,False:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vehicle.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure to load the file all the way to prepped\n",
    "\n",
    "reload(kc)\n",
    "veh_aux=kc.getalldata('veh_aux.csv')  # load all year's data and add id\n",
    "veh_aux=kc.removeUnneededColumns(veh_aux,'veh_aux',['veh_no'])  # keep veh_no\n",
    "#veh_aux=kc.drop([],axis=1) # drop additional columns from deeper EDA for lack of variance ien the variables\n",
    "veh_aux=veh_aux.loc[(veh_aux.veh_no<=4),:]    # filter only veh_no <=4\n",
    "veh_aux=kc.translateVarsFromDict(veh_aux,kc.createDataDictForTranslation('veh_aux'))   #remap the values from FARS to ours own values\n",
    "veh_aux['veh_no']=['veh'+str(int(i)) for i in veh_aux.veh_no]    # make veh_no a string instead\n",
    "veh_aux=kc.pivot_and_chunk(veh_aux)    # pivot the veh_no from values into columns headers (expanding number of columns by columns * veh_no.unique()\n",
    "cols=veh_aux.columns.tolist()          # Filtering out columns to binarized -- only categorical columns\n",
    "for k,v in kc.createDataDictForTranslation().items():\n",
    "    if k in kc.checkDataDictForType(SourceFile='veh_aux',return_type='numeric'):\n",
    "        for i in range(1,5):\n",
    "            cols.remove(k+'$veh'+str(i))\n",
    "veh_aux=kc.binarizeVariables(veh_aux,cols,500)    # binarize the columns\n",
    "veh_aux=kc.addSummaryStats(veh_aux,functions=[np.nanmean,np.nansum,np.any],postfixes=['mean','sum','any']).replace({True:1,False:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_drdro$veh1_No</th>\n",
       "      <th>a_drdro$veh1_Drowsy Driver</th>\n",
       "      <th>a_drdro$veh2_No</th>\n",
       "      <th>a_drdro$veh2_Drowsy Driver</th>\n",
       "      <th>a_drdro$veh3_No</th>\n",
       "      <th>a_drdro$veh3_Drowsy Driver</th>\n",
       "      <th>a_drdro$veh4_No</th>\n",
       "      <th>a_drdro$veh4_Drowsy Driver</th>\n",
       "      <th>a_imp1$veh1_Front 26,495 28,055 27,753 27,373 29,963</th>\n",
       "      <th>a_imp1$veh1_Non-Collision 3,442 3,426 3,144 3,084 3,262</th>\n",
       "      <th>...</th>\n",
       "      <th>a_imp1$Left Side 4,511 4,590 4,369 4,504 4,841$any</th>\n",
       "      <th>a_imp1$Non-Collision 3,442 3,426 3,144 3,084 3,262$mean</th>\n",
       "      <th>a_imp1$Non-Collision 3,442 3,426 3,144 3,084 3,262$sum</th>\n",
       "      <th>a_imp1$Non-Collision 3,442 3,426 3,144 3,084 3,262$any</th>\n",
       "      <th>a_lic_s$Invalid$mean</th>\n",
       "      <th>a_lic_s$Invalid$sum</th>\n",
       "      <th>a_lic_s$Invalid$any</th>\n",
       "      <th>a_lic_s$Not Applicable$mean</th>\n",
       "      <th>a_lic_s$Not Applicable$sum</th>\n",
       "      <th>a_lic_s$Not Applicable$any</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_2014.100001</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_2014.100002</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_2014.100003</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_2014.100004</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_2014.100005</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                a_drdro$veh1_No  a_drdro$veh1_Drowsy Driver  a_drdro$veh2_No  \\\n",
       "id                                                                             \n",
       "id_2014.100001                1                           0                1   \n",
       "id_2014.100002                1                           0                1   \n",
       "id_2014.100003                1                           0                1   \n",
       "id_2014.100004                1                           0                1   \n",
       "id_2014.100005                1                           0                0   \n",
       "\n",
       "                a_drdro$veh2_Drowsy Driver  a_drdro$veh3_No  \\\n",
       "id                                                            \n",
       "id_2014.100001                           0                0   \n",
       "id_2014.100002                           0                0   \n",
       "id_2014.100003                           0                0   \n",
       "id_2014.100004                           0                0   \n",
       "id_2014.100005                           0                0   \n",
       "\n",
       "                a_drdro$veh3_Drowsy Driver  a_drdro$veh4_No  \\\n",
       "id                                                            \n",
       "id_2014.100001                           0                0   \n",
       "id_2014.100002                           0                0   \n",
       "id_2014.100003                           0                0   \n",
       "id_2014.100004                           0                0   \n",
       "id_2014.100005                           0                0   \n",
       "\n",
       "                a_drdro$veh4_Drowsy Driver  \\\n",
       "id                                           \n",
       "id_2014.100001                           0   \n",
       "id_2014.100002                           0   \n",
       "id_2014.100003                           0   \n",
       "id_2014.100004                           0   \n",
       "id_2014.100005                           0   \n",
       "\n",
       "                a_imp1$veh1_Front 26,495 28,055 27,753 27,373 29,963  \\\n",
       "id                                                                     \n",
       "id_2014.100001                                                  0      \n",
       "id_2014.100002                                                  1      \n",
       "id_2014.100003                                                  0      \n",
       "id_2014.100004                                                  1      \n",
       "id_2014.100005                                                  1      \n",
       "\n",
       "                a_imp1$veh1_Non-Collision 3,442 3,426 3,144 3,084 3,262  ...  \\\n",
       "id                                                                       ...   \n",
       "id_2014.100001                                                  0        ...   \n",
       "id_2014.100002                                                  0        ...   \n",
       "id_2014.100003                                                  0        ...   \n",
       "id_2014.100004                                                  0        ...   \n",
       "id_2014.100005                                                  0        ...   \n",
       "\n",
       "                a_imp1$Left Side 4,511 4,590 4,369 4,504 4,841$any  \\\n",
       "id                                                                   \n",
       "id_2014.100001                                                  0    \n",
       "id_2014.100002                                                  0    \n",
       "id_2014.100003                                                  1    \n",
       "id_2014.100004                                                  0    \n",
       "id_2014.100005                                                  0    \n",
       "\n",
       "                a_imp1$Non-Collision 3,442 3,426 3,144 3,084 3,262$mean  \\\n",
       "id                                                                        \n",
       "id_2014.100001                                                0.0         \n",
       "id_2014.100002                                                0.0         \n",
       "id_2014.100003                                                0.0         \n",
       "id_2014.100004                                                0.0         \n",
       "id_2014.100005                                                0.0         \n",
       "\n",
       "                a_imp1$Non-Collision 3,442 3,426 3,144 3,084 3,262$sum  \\\n",
       "id                                                                       \n",
       "id_2014.100001                                                  0        \n",
       "id_2014.100002                                                  0        \n",
       "id_2014.100003                                                  0        \n",
       "id_2014.100004                                                  0        \n",
       "id_2014.100005                                                  0        \n",
       "\n",
       "                a_imp1$Non-Collision 3,442 3,426 3,144 3,084 3,262$any  \\\n",
       "id                                                                       \n",
       "id_2014.100001                                                  0        \n",
       "id_2014.100002                                                  0        \n",
       "id_2014.100003                                                  0        \n",
       "id_2014.100004                                                  0        \n",
       "id_2014.100005                                                  0        \n",
       "\n",
       "                a_lic_s$Invalid$mean  a_lic_s$Invalid$sum  \\\n",
       "id                                                          \n",
       "id_2014.100001                  0.25                    1   \n",
       "id_2014.100002                  0.00                    0   \n",
       "id_2014.100003                  0.25                    1   \n",
       "id_2014.100004                  0.25                    1   \n",
       "id_2014.100005                  0.00                    0   \n",
       "\n",
       "                a_lic_s$Invalid$any  a_lic_s$Not Applicable$mean  \\\n",
       "id                                                                 \n",
       "id_2014.100001                    1                          0.0   \n",
       "id_2014.100002                    0                          0.0   \n",
       "id_2014.100003                    1                          0.0   \n",
       "id_2014.100004                    1                          0.0   \n",
       "id_2014.100005                    0                          0.0   \n",
       "\n",
       "                a_lic_s$Not Applicable$sum  a_lic_s$Not Applicable$any  \n",
       "id                                                                      \n",
       "id_2014.100001                           0                           0  \n",
       "id_2014.100002                           0                           0  \n",
       "id_2014.100003                           0                           0  \n",
       "id_2014.100004                           0                           0  \n",
       "id_2014.100005                           0                           0  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#veh_aux.head()\n",
    "veh_aux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "131585\n"
     ]
    }
   ],
   "source": [
    "print(len(vehicle.reset_index().id.unique())==vehicle.shape[0])\n",
    "print(vehicle.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "131585\n"
     ]
    }
   ],
   "source": [
    "print(len(veh_aux.reset_index().id.unique())==veh_aux.shape[0])\n",
    "print(veh_aux.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with accident file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident=kc.getalldata()\n",
    "accident=kc.removeNoHourAndMinutes(accident)\n",
    "accident=kc.cleanLocs(accident)\n",
    "accident=kc.createTimestamp(accident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident=kc.removeUnneededColumns(accident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129979, 19)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accident.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident=accident.set_index('id')\n",
    "accident=accident.join(vehicle,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident=accident.join(veh_aux,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129979, 1302)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accident.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicates\n",
    "any(accident.reset_index().id.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident.to_parquet('kc_accident_vehicle_veh_aux.parquet')\n",
    "accident.to_csv('kc_accident_vehicle_veh_aux.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create function so it vertically aligns the \"any\"s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kc_functions as kc\n",
    "accident=pd.read_parquet('kc_accident_vehicle_veh_aux.parquet')\n",
    "accident=accident.loc[:,[i for i in accident.columns.tolist() if re.search('.*?\\$.*?\\$any$',i)]]\n",
    "# make accident in long form, break out the 2 variable$category into 2 columns, sort it\n",
    "accident=accident.reset_index().melt(id_vars='id',var_name='full').assign(variable=lambda x: [ re.search('(.*?)(?:\\$.*$)',i).group(1) for i in x.full],\n",
    "                                                                         category=lambda x: [re.search('(?:.*?\\$)(.*?)(?:\\$any$)',i).group(1) for i in x.full]) \\\n",
    "                .sort_values(by='id') # .drop('full',axis=1)\n",
    "# move category of the variable back to columns and use value as the values\n",
    "# accident=accident.assign(idcat=lambda x: str(x.id) + '$'+ str(x.variable)) \\\n",
    "#                 .drop(['id','variable'],axis=1).set_index('idcat')\n",
    "# accident=accident.pivot(columns='category',values='value').to_sparse().reset_index()\n",
    "# accident=accident.assign(id=lambda x: [ re.search('(^id.*?)(?:\\$.*)',i).group(1) for i in x.idcat],\n",
    "#                        category=lambda x: [re.search('(?:^id.*?\\$)(.*$)',i).group(1) for i in x.idcat]).drop('idcat',axis=1)\n",
    "# accident.to_parquet('kc_accident_vehicle_veh_aux_filtered_any.parquet')\n",
    "accident.to_csv('kc_accident_vehicle_veh_aux_filtered_any.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a list of variable where sd > 0 from any's (ie. it's not all the same number) - add 'state' to it. The output csv is used to help the creation of the Correlations Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib  import reload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kc_functions as kc\n",
    "import re\n",
    "\n",
    "#df=pd.read_csv('kc_tmp.csv')\n",
    "df=pd.read_parquet('kc_accident_vehicle_veh_aux.parquet')\n",
    "z=df.loc[:,[i for i in df.columns.tolist() if re.search('any$',i)]]\n",
    "k=z.apply(np.std,axis=0)\n",
    "c=k[k>0].index.tolist()\n",
    "c.append('state')\n",
    "df.loc[:,c].to_csv('kc_tmp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to caculate Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare ,kendalltau\n",
    "\n",
    "def corrmatrix(df,correlation_function,ret=0,**kwargs):\n",
    "    '''\n",
    "    given the dataframe, df, go through each column and calculate the correlation_function (chisquare, kendall's Tau). Return the 'ret' # argument\n",
    "    from the results of the correlation function - for example, chi-statistics, p-value = chisquare(...) -> to get hte p-value, ret should be 1.\n",
    "    correlation, p-value = kendalltau(...) --> to get the corraltion, the ret should be 0.\n",
    "    \n",
    "    **kwargs is passed on to the correlation function for additoinal parameters.\n",
    "    \n",
    "    '''\n",
    "    cols=df.columns.tolist()\n",
    "    matrix=pd.DataFrame([],index=cols,columns=cols)\n",
    "    for c in cols:\n",
    "        for r in cols:\n",
    "            if c!=r:\n",
    "                x=df.loc[(~df[c].isnull()) & (~df[r].isnull()),c].astype('int')\n",
    "                y=df.loc[(~df[c].isnull()) & (~df[r].isnull()),r].astype('int')\n",
    "                if correlation_function == kendalltau:\n",
    "                    q=correlation_function(x,y,**kwargs)\n",
    "                else:\n",
    "                    q=correlation_function(x,y)\n",
    "                matrix.loc[r,c]=1 if np.isnan(q[ret]) else q[ret]    # the if... then statement is needed bcs scipy with p-value = 1 caculates a nan instead\n",
    "    return pd.DataFrame(matrix,index=cols,columns=cols)\n",
    "\n",
    "def calcCorrMatrix(df,groupby_var='state',kwargs={'correlation_function':chisquare,'nan_policy':'omit','ret':1}):\n",
    "    '''\n",
    "    Calcaulate the correlation matrix using corrmatrix(...) function for the entire dataframe. Then group by the groupby_var and\n",
    "    calculate the correlation matrix for each group. kwargs is a dictionary specifying the correlation function, etc.\n",
    "    '''\n",
    "    # calc for all\n",
    "    dfprim=corrmatrix(df,**kwargs).reset_index().rename({'index':'prim_var'},axis=1).melt(id_vars='prim_var') \\\n",
    "           .assign(state=0,full=lambda x: [i + '-' + j for i,j in zip(x.prim_var,x.variable)])\n",
    "    \n",
    "    for i in set(df[groupby_var]):\n",
    "        dftmp=pd.DataFrame(df.loc[df[groupby_var]==i,:])\n",
    "        dftmp=corrmatrix(dftmp,**kwargs).reset_index().rename({'index':'prim_var'},axis=1).melt(id_vars='prim_var') \\\n",
    "           .assign(state=i,full=lambda x: [p + '-' + q for p,q in zip(x.prim_var,x.variable)])\n",
    "        dfprim=dfprim.append(dftmp)\n",
    "    return dfprim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=pd.read_csv('kc_tmp.csv').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=calcCorrMatrix(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.to_csv('kc_corr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=pd.read_csv('kc_tmp.csv').set_index('id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=calcCorrMatrix(z,groupby_var='state',kwargs={'correlation_function':chisquare,'ret':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9097959895689501"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=accident.loc[(~accident['a_lic_s$Valid$any'].isnull()) & (~accident['deformed$Other$any'].isnull()),'a_lic_s$Valid$any'].astype('int')\n",
    "y=accident.loc[(~accident['a_lic_s$Valid$any'].isnull()) & (~accident['deformed$Other$any'].isnull()),'deformed$Other$any'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=-0.09840902565876788, pvalue=1.0612304959100567e-275)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendalltau(x,y,nan_policy='omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.loc['a_lic_s$Valid$any','deformed$Other$any']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/kchuangk/.pyenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import kc_functions as kc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.greedy=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accident.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accident.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accident.loc[:,[True if re.search('a_drdro',i) else False for i in accident.columns.tolist()]].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=accident.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('acc_type', 'Goes a bit crazy, top 5')"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('(.*)\\$veh[1-4]_{0,1}(.*)','acc_type$veh3_Goes a bit crazy, top 5').groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=pd.DataFrame([[1,2,3],[4,5,6]],columns=['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  4  5  6"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, b, c]\n",
       "1    [a, b, c]\n",
       "dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.apply(lambda x: x.index.tolist(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('reg_stat',)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('(.*)\\$veh[1-4]$','reg_stat$veh2').groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(kc)\n",
    "a=accident.head(1).apply(kc.addSummaryStats,axis=1,functions=[np.sum],postfixes=['sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numpy.any(a, axis=None, out=None, keepdims=<no value>)>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=accident.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(kc)\n",
    "# a=kc.addSummaryStats(r,[np.nansum,np.nanmean,np.any],['sum','mean','any'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numoccs$veh1        2\n",
       "numoccs$veh2      NaN\n",
       "numoccs$veh3      NaN\n",
       "numoccs$veh4      NaN\n",
       "numoccs$mean        2\n",
       "numoccs$sum         2\n",
       "numoccs$any      True\n",
       "numoccs$$sum        2\n",
       "numoccs$$mean       2\n",
       "numoccs$$any     True\n",
       "Name: id_2014.10001, dtype: object"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[True if re.search('numoccs',i) else False for i in a.index.tolist()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nansum(a.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(a.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0744573456182584e+18"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(np.array([np.nan,10,10]).astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.Series([np.nan,0,1,True,10,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.index=['a','b','c','d','e','f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a      NaN\n",
       "b        0\n",
       "c        1\n",
       "d     True\n",
       "e       10\n",
       "f    False\n",
       "dtype: object"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(a.dropna().astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b        0\n",
       "c        1\n",
       "d     True\n",
       "e       10\n",
       "f    False\n",
       "dtype: object"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Testing', 'stuff')"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('(Testing) (?!the)this (stuff)','Going Testing this stuff...').groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
